{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import operator\n",
    "import string\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk.data import find\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "nltk_words = set([x.lower() for x in words.words()])\n",
    "json_dict={}\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def wordLemmatizer(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma    \n",
    "def max_match(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    for i in range(len(s), 0, -1):\n",
    "        firstword = wordLemmatizer(s[:i])\n",
    "        remainer = s[i:]\n",
    "        if firstword in nltk_words:\n",
    "            return [firstword] + max_match(remainer)\n",
    "    firstword = s[0]\n",
    "    remainer = s[1:]\n",
    "    return [firstword] + max_match(remainer)\n",
    " \n",
    "\n",
    "def preprocess(s, lowercase=True):\n",
    "    hash_tags = [str(x) for x in re.findall(r\"#\\w+\", s)]\n",
    "    cleaned_tags = []\n",
    "    for hash_tag in hash_tags:\n",
    "        hash_tag = hash_tag.replace(\"#\",\"\")\n",
    "        words = re.split(r'([A-Z][a-z]*)', hash_tag)\n",
    "        if len(words) > 1 :\n",
    "            cleaned_tags += [x.lower() for x in words if x] # remove empty strings\n",
    "        elif len(words) == 1:\n",
    "            cleaned_tags += max_match(words[0])\n",
    "    hashString=' '.join(cleaned_tags)\n",
    "    #remove url hashtag and @\n",
    "    s = re.sub(r'(https?:\\/\\/[a-zA-Z0-9.\\/\\?=#]*|#\\w+|@\\w+)', '', s, flags=re.MULTILINE)\n",
    "    s=' '.join((s,hashString))\n",
    "    s=s.lower()\n",
    "    #s = ''.join(ch for ch in s if ch not in punctuation)\n",
    "    tokens = tokenize(s)\n",
    "    tokens = [wordLemmatizer(x) for x in tokens]\n",
    "    tokens = [ word for word in tokens if word not in stop ]\n",
    "    return tokens\n",
    "\n",
    "    \n",
    "# tweet = \"RT @marcobonzanini: just an example! :D http://example.com #NLP\"\n",
    "# print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n",
    "\n",
    "def polarity_classification(tweetTokens, positive_words_set, negative_words_set):\n",
    "    '''\n",
    "    rtype: [int]\n",
    "    '''\n",
    "    sentimentPredictions = []\n",
    "    positiveCount = 0\n",
    "    negativeCount = 0\n",
    "   \n",
    "    for word in tweetTokens:\n",
    "        if word in positive_words_set:\n",
    "            positiveCount += 1\n",
    "        elif word in negative_words_set:\n",
    "            negativeCount += 1\n",
    "\n",
    "    if positiveCount > negativeCount:\n",
    "        polarity_score=1\n",
    "    elif positiveCount <negativeCount:\n",
    "        polarity_score=-1\n",
    "    else:\n",
    "        polarity_score=0\n",
    "    return polarity_score        \n",
    "\n",
    "def download_sample_nltk_corpus():\n",
    "    nltk.download('word2vec_sample')\n",
    "\n",
    "\n",
    "#\n",
    "#Tokenizing the processed tweets (tweet text)\n",
    "#\n",
    "def posNegWordList():\n",
    "    positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "    negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "\n",
    "    word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "    positive_words2 = []\n",
    "    negative_words2 = []\n",
    "\n",
    "    for word in model.vocab:\n",
    "        positive_score = 0.0\n",
    "        negative_score = 0.0\n",
    "        for seed in positive_seeds:\n",
    "            positive_score += model.similarity(word, seed)\n",
    "        for seed in negative_seeds:\n",
    "            negative_score += model.similarity(word, seed)\n",
    "        overall_score = (positive_score  - negative_score) / (len(negative_seeds) + len(positive_seeds))\n",
    "        if overall_score < -0.03:\n",
    "            negative_words2.append(word)\n",
    "        if overall_score > 0.03:\n",
    "            positive_words2.append(word)\n",
    "    return positive_words2, negative_words2\n",
    " \n",
    "# if tweet contains emoticons no tweet analysis otherwise apply sentiment analysis.\n",
    "def processSentimentAnnlysis(tweet_text,positive_words,negative_words):\n",
    "    # due to lower case :D convert to :d\n",
    "    positive_words2=positive_words\n",
    "    negative_words2=negative_words\n",
    "    patternSmile = re.compile(\"^(\\|?>?[:*;Xx8=]-?o?\\^?[DdPpb3)}\\]>]\\)?)$\")\n",
    "    patternSad = re.compile(\"^(([:><].?-?[@><cC(\\[{\\|]\\|?|[Dd][:8;=X]<?|v.v))$\")\n",
    "    #tweet = json.loads(line)\n",
    "    tweet=tweet_text\n",
    "    #tokens = preprocess(tweet['text'])\n",
    "    tokens = preprocess(tweet)\n",
    "    smiley=False\n",
    "    for x in tokens:\n",
    "        try:\n",
    "            if patternSmile.match(x):\n",
    "                polarity_final_score=1  \n",
    "                smiley=True\n",
    "                break\n",
    "            elif patternSad.match(x):\n",
    "                polarity_final_score=-1\n",
    "                smiley=True\n",
    "                break   \n",
    "        except:\n",
    "            print ('')\n",
    "    if not smiley :\n",
    "        tokens = [''.join(char for char in stringToken if char not in punctuation) for stringToken in tokens]\n",
    "        polarity_score=polarity_classification(tokens, set(positive_words2), set(negative_words2))\n",
    "        polarity_final_score=polarity_score\n",
    "    return polarity_final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adding_polarityScore(fileName):\n",
    "    filename=fileName\n",
    "    positive_words2,negative_words2 = posNegWordList()\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename)\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        polarity_final_score=processSentimentAnnlysis(tweet_dict[\"value\"][\"text\"],positive_words2,negative_words2)\n",
    "        print tweet_dict[\"value\"][\"text\"]\n",
    "        print polarity_final_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adding_polarityScore(\"tweetMelbourne.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named MySQLdb",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ead6a7853dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#!/usr/bin/python\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mMySQLdb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m: No module named MySQLdb"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import MySQLdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
