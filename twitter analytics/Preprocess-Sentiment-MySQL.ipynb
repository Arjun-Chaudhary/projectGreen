{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import words\n",
    "import re\n",
    "import json\n",
    "from collections import Counter\n",
    "import operator\n",
    "import string\n",
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "from nltk.data import find\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)\n",
    "nltk_words = set([x.lower() for x in words.words()])\n",
    "json_dict={}\n",
    "\n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    "\n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    "\n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "\n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    "\n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    "\n",
    "def wordLemmatizer(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma    \n",
    "def max_match(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    for i in range(len(s), 0, -1):\n",
    "        firstword = wordLemmatizer(s[:i])\n",
    "        remainer = s[i:]\n",
    "        if firstword in nltk_words:\n",
    "            return [firstword] + max_match(remainer)\n",
    "    firstword = s[0]\n",
    "    remainer = s[1:]\n",
    "    return [firstword] + max_match(remainer)\n",
    " \n",
    "\n",
    "def preprocess(s, lowercase=True):\n",
    "    hash_tags = [str(x) for x in re.findall(r\"#\\w+\", s)]\n",
    "    cleaned_tags = []\n",
    "    for hash_tag in hash_tags:\n",
    "        hash_tag = hash_tag.replace(\"#\",\"\")\n",
    "        words = re.split(r'([A-Z][a-z]*)', hash_tag)\n",
    "        if len(words) > 1 :\n",
    "            cleaned_tags += [x.lower() for x in words if x] # remove empty strings\n",
    "        elif len(words) == 1:\n",
    "            cleaned_tags += max_match(words[0])\n",
    "    hashString=' '.join(cleaned_tags)\n",
    "    #remove url hashtag and @\n",
    "    s = re.sub(r'(https?:\\/\\/[a-zA-Z0-9.\\/\\?=#]*|#\\w+|@\\w+)', '', s, flags=re.MULTILINE)\n",
    "    s=' '.join((s,hashString))\n",
    "    s=s.lower()\n",
    "    #s = ''.join(ch for ch in s if ch not in punctuation)\n",
    "    tokens = tokenize(s)\n",
    "    tokens = [wordLemmatizer(x) for x in tokens]\n",
    "    tokens = [ word for word in tokens if word not in stop ]\n",
    "    return tokens\n",
    "\n",
    "    \n",
    "# tweet = \"RT @marcobonzanini: just an example! :D http://example.com #NLP\"\n",
    "# print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']\n",
    "\n",
    "def polarity_classification(tweetTokens, positive_words_set, negative_words_set):\n",
    "    '''\n",
    "    rtype: [int]\n",
    "    '''\n",
    "    sentimentPredictions = []\n",
    "    positiveCount = 0\n",
    "    negativeCount = 0\n",
    "   \n",
    "    for word in tweetTokens:\n",
    "        if word in positive_words_set:\n",
    "            positiveCount += 1\n",
    "        elif word in negative_words_set:\n",
    "            negativeCount += 1\n",
    "\n",
    "    if positiveCount > negativeCount:\n",
    "        polarity_score=1\n",
    "    elif positiveCount <negativeCount:\n",
    "        polarity_score=-1\n",
    "    else:\n",
    "        polarity_score=0\n",
    "    return polarity_score        \n",
    "\n",
    "def download_sample_nltk_corpus():\n",
    "    nltk.download('word2vec_sample')\n",
    "\n",
    "\n",
    "#\n",
    "#Tokenizing the processed tweets (tweet text)\n",
    "#\n",
    "def posNegWordList():\n",
    "    positive_seeds = [\"good\",\"nice\",\"excellent\",\"positive\",\"fortunate\",\"correct\",\"superior\",\"great\"]\n",
    "    negative_seeds = [\"bad\",\"nasty\",\"poor\",\"negative\",\"unfortunate\",\"wrong\",\"inferior\",\"awful\"]\n",
    "\n",
    "\n",
    "    word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "    model = gensim.models.Word2Vec.load_word2vec_format(word2vec_sample, binary=False)\n",
    "    positive_words2 = []\n",
    "    negative_words2 = []\n",
    "\n",
    "    for word in model.vocab:\n",
    "        positive_score = 0.0\n",
    "        negative_score = 0.0\n",
    "        for seed in positive_seeds:\n",
    "            positive_score += model.similarity(word, seed)\n",
    "        for seed in negative_seeds:\n",
    "            negative_score += model.similarity(word, seed)\n",
    "        overall_score = (positive_score  - negative_score) / (len(negative_seeds) + len(positive_seeds))\n",
    "        if overall_score < -0.03:\n",
    "            negative_words2.append(word)\n",
    "        if overall_score > 0.03:\n",
    "            positive_words2.append(word)\n",
    "    return positive_words2, negative_words2\n",
    " \n",
    "# if tweet contains emoticons no tweet analysis otherwise apply sentiment analysis.\n",
    "def processSentimentAnnlysis(tweet_text,positive_words,negative_words):\n",
    "    # due to lower case :D convert to :d\n",
    "    positive_words2=positive_words\n",
    "    negative_words2=negative_words\n",
    "    patternSmile = re.compile(\"^(\\|?>?[:*;Xx8=]-?o?\\^?[DdPpb3)}\\]>]\\)?)$\")\n",
    "    patternSad = re.compile(\"^(([:><].?-?[@><cC(\\[{\\|]\\|?|[Dd][:8;=X]<?|v.v))$\")\n",
    "    #tweet = json.loads(line)\n",
    "    tweet=tweet_text\n",
    "    #tokens = preprocess(tweet['text'])\n",
    "    tokens = preprocess(tweet)\n",
    "    smiley=False\n",
    "    for x in tokens:\n",
    "        try:\n",
    "            if patternSmile.match(x):\n",
    "                polarity_final_score=1  \n",
    "                smiley=True\n",
    "                break\n",
    "            elif patternSad.match(x):\n",
    "                polarity_final_score=-1\n",
    "                smiley=True\n",
    "                break   \n",
    "        except:\n",
    "            print ('')\n",
    "    if not smiley :\n",
    "        tokens = [''.join(char for char in stringToken if char not in punctuation) for stringToken in tokens]\n",
    "        polarity_score=polarity_classification(tokens, set(positive_words2), set(negative_words2))\n",
    "        polarity_final_score=polarity_score\n",
    "    return polarity_final_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_db_config(filename, section):\n",
    "    \"\"\" Read database configuration file and return a dictionary object\n",
    "    :param filename: name of the configuration file\n",
    "    :param section: section of database configuration\n",
    "    :return: a dictionary of database parameters\n",
    "    \"\"\"\n",
    "    # create parser and read ini configuration file\n",
    "    parser = ConfigParser()\n",
    "    parser.read(filename)\n",
    " \n",
    "    # get section, default to mysql\n",
    "    db = {}\n",
    "    if parser.has_section(section):\n",
    "        items = parser.items(section)\n",
    "        for item in items:\n",
    "            db[item[0]] = item[1]\n",
    "    else:\n",
    "        raise Exception('{0} not found in the {1} file'.format(section, filename))\n",
    " \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "read_db_config('config.ini','mysql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mysql.connector import MySQLConnection, Error\n",
    " \n",
    "def insert_tweetMelbourne(conn,tweet_id,created_at,geo_type,geo_coordinates_latitude,geo_coordinates_longitude,place_full_name,\n",
    "                          place_country,place_type,place_bounding_box_type,place_bounding_box_coordinates_NE_lat,\n",
    "                          place_bounding_box_coordinates_NE_long,place_bounding_box_coordinates_SW_lat,\n",
    "                          place_bounding_box_coordinates_SW_long,place_country_code,place_name,text,user_id,user_verified,\n",
    "                          user_followers_count,user_listed_count,user_friends_count,user_location,user_following,\n",
    "                          user_geo_enabled,user_lang,polarity_score):\n",
    "    query = \"INSERT INTO tweetMelbourne(tweet_id,created_at,geo_type,geo_coordinates_latitude,geo_coordinates_longitude,place_full_name,\"\\\n",
    "            \"place_country,place_type,place_bounding_box_type,place_bounding_box_coordinates_NE_lat,\"\\\n",
    "            \"place_bounding_box_coordinates_NE_long,place_bounding_box_coordinates_SW_lat,\"\\\n",
    "            \"place_bounding_box_coordinates_SW_long,place_country_code,place_name,text,user_id,user_verified,\"\\\n",
    "            \"user_followers_count,user_listed_count,user_friends_count,user_location,user_following,\"\\\n",
    "            \"user_geo_enabled,user_lang,polarity_score) \" \\\n",
    "            \"VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)\"\n",
    "    args = (tweet_id,created_at,geo_type,geo_coordinates_latitude,geo_coordinates_longitude,place_full_name,\n",
    "            place_country,place_type,place_bounding_box_type,place_bounding_box_coordinates_NE_lat,\n",
    "            place_bounding_box_coordinates_NE_long,place_bounding_box_coordinates_SW_lat,\n",
    "            place_bounding_box_coordinates_SW_long,place_country_code,place_name,text,user_id,user_verified,\n",
    "            user_followers_count,user_listed_count,user_friends_count,user_location,user_following,\n",
    "            user_geo_enabled,user_lang,polarity_score)\n",
    " \n",
    "    try:        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(query, args)\n",
    " \n",
    "        if cursor.lastrowid%10000==0:\n",
    "            print('last insert id', cursor.lastrowid)\n",
    " \n",
    "        conn.commit()\n",
    "    except Error as error:\n",
    "        print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def add_data(fileName):\n",
    "    filename=fileName\n",
    "    positive_words2,negative_words2 = posNegWordList()\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    f = open(filename)\n",
    "    print('Connecting to MySQL database...')\n",
    "    conn = mysql.connector.connect(host='localhost',\n",
    "                                       database='aircasting_development',\n",
    "                                       user='root',\n",
    "                                       password='')\n",
    "    if conn.is_connected():\n",
    "        print('connection established.')\n",
    "    else:\n",
    "        print('connection failed.')\n",
    "    for line in f:\n",
    "        tweet_dict = json.loads(line)\n",
    "        polarity_final_score=processSentimentAnnlysis(tweet_dict[\"value\"][\"text\"],positive_words2,negative_words2)\n",
    "        try:\n",
    "            tweet_id=tweet_dict[\"id\"]\n",
    "            created_at=time.strftime('%Y-%m-%d %H:%M:%S', time.strptime(tweet_dict[\"value\"][\"created_at\"],'%a %b %d %H:%M:%S +0000 %Y'))\n",
    "            geo_type= tweet_dict[\"value\"][\"geo\"][\"type\"]\n",
    "            geo_coordinates_latitude= tweet_dict[\"value\"][\"geo\"][\"coordinates\"][0]\n",
    "            geo_coordinates_longitude= tweet_dict[\"value\"][\"geo\"][\"coordinates\"][1]\n",
    "            place_full_name= tweet_dict[\"value\"][\"place\"][\"full_name\"]\n",
    "            place_country= tweet_dict[\"value\"][\"place\"][\"country\"]\n",
    "            place_type= tweet_dict[\"value\"][\"place\"][\"place_type\"]\n",
    "            place_bounding_box_type= tweet_dict[\"value\"][\"place\"][\"bounding_box\"][\"type\"]\n",
    "            # North East (lat, long)\n",
    "            place_bounding_box_coordinates_NE_lat= tweet_dict[\"value\"][\"place\"][\"bounding_box\"][\"coordinates\"][0][0][1]\n",
    "            place_bounding_box_coordinates_NE_long= tweet_dict[\"value\"][\"place\"][\"bounding_box\"][\"coordinates\"][0][0][0]\n",
    "            # South West (lat,long)\n",
    "            place_bounding_box_coordinates_SW_lat= tweet_dict[\"value\"][\"place\"][\"bounding_box\"][\"coordinates\"][0][2][1]\n",
    "            place_bounding_box_coordinates_SW_long= tweet_dict[\"value\"][\"place\"][\"bounding_box\"][\"coordinates\"][0][2][0]\n",
    "            place_country_code= tweet_dict[\"value\"][\"place\"][\"country_code\"]\n",
    "            place_name= tweet_dict[\"value\"][\"place\"][\"name\"]\n",
    "            text= tweet_dict[\"value\"][\"text\"]\n",
    "            user_id= tweet_dict[\"value\"][\"user\"][\"id\"]\n",
    "            user_verified= tweet_dict[\"value\"][\"user\"][\"verified\"]\n",
    "            user_followers_count= tweet_dict[\"value\"][\"user\"][\"followers_count\"]\n",
    "            user_listed_count= tweet_dict[\"value\"][\"user\"][\"listed_count\"]\n",
    "            user_friends_count= tweet_dict[\"value\"][\"user\"][\"friends_count\"]\n",
    "            user_location= tweet_dict[\"value\"][\"user\"][\"location\"]\n",
    "            user_following= tweet_dict[\"value\"][\"user\"][\"following\"]\n",
    "            user_geo_enabled= tweet_dict[\"value\"][\"user\"][\"geo_enabled\"]\n",
    "            user_lang= tweet_dict[\"value\"][\"user\"][\"lang\"]\n",
    "            polarity_score= polarity_final_score\n",
    "        except:\n",
    "            pass    \n",
    "        insert_tweetMelbourne(conn,tweet_id,created_at,geo_type,geo_coordinates_latitude,geo_coordinates_longitude,place_full_name,\n",
    "                              place_country,place_type,place_bounding_box_type,place_bounding_box_coordinates_NE_lat,\n",
    "                              place_bounding_box_coordinates_NE_long,place_bounding_box_coordinates_SW_lat,\n",
    "                              place_bounding_box_coordinates_SW_long,place_country_code,place_name,text,user_id,user_verified,\n",
    "                              user_followers_count,user_listed_count,user_friends_count,user_location,user_following,\n",
    "                              user_geo_enabled,user_lang,polarity_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to MySQL database...\n",
      "connection established.\n",
      "('last insert id', 10000)\n",
      "('last insert id', 20000)\n",
      "('last insert id', 30000)\n",
      "('last insert id', 40000)\n",
      "('last insert id', 50000)\n",
      "('last insert id', 60000)\n",
      "('last insert id', 70000)\n",
      "('last insert id', 80000)\n",
      "('last insert id', 90000)\n",
      "('last insert id', 100000)\n"
     ]
    }
   ],
   "source": [
    "add_data('tweet1Million.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-179-cf71dd30d3a1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-179-cf71dd30d3a1>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    (Canopy 64bit) C:\\xampp\\htdocs\\projGreen_ver1\\twitter analytics>pip install https://cdn.mysql.com/Download\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "(Canopy 64bit) C:\\xampp\\htdocs\\projGreen_ver1\\twitter analytics>pip install https://cdn.mysql.com/Download\n",
    "s/Connector-Python/mysql-connector-python-1.0.12.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(u'538838165868797953', 1, u'Point', datetime.datetime(2014, 11, 29, 23, 33, 24), Decimal('-37.666521920'), Decimal('144.846125430'), u'Melbourne Airport, Melbourne', u'Australia', u'neighborhood', u'Polygon', Decimal('-37.710380970'), Decimal('144.797614920'), Decimal('-37.641009960'), Decimal('144.874238040'), u'AU', u'Melbourne Airport', u'wheels down, Oz. #gdaymate ????????????', 1081, u'1', 248876, 8891, 13755, u'Silicon Valley, CA', u'0', u'1', u'en', 0)\n",
      "(u'539330441057013760', 2, u'Point', datetime.datetime(2014, 12, 1, 8, 9, 32), Decimal('-37.823909770'), Decimal('144.991198050'), u'Melbourne, Victoria', u'Australia', u'city', u'Polygon', Decimal('-38.433859306'), Decimal('144.593741856'), Decimal('-37.511273723'), Decimal('145.512528832'), u'AU', u'Melbourne', u'hello Melbourne geeks / Startup Victoria! cc @ga @AngelCubeMelb @500Startups http://t.co/x2YIEunWUc', 1081, u'1', 249055, 8893, 13761, u'Silicon Valley, CA', u'0', u'1', u'en', 1)\n"
     ]
    }
   ],
   "source": [
    "from mysql.connector import MySQLConnection, Error\n",
    "conn = mysql.connector.connect(host= \"localhost\",\n",
    "                  user=\"root\",\n",
    "                  passwd=\"\",\n",
    "                  db=\"aircasting_development\")\n",
    "x = conn.cursor()\n",
    "\n",
    "try:\n",
    " \n",
    "  # query = \"\"\"INSERT INTO tweetMelbourne(created_at,geo_coordinates_latitude,geo_coordinates_longitude,user_id,polarity_score) \\\n",
    "    #        VALUES(%s,%s,%s,%s,%s)\"\"\"\n",
    "   #args = ('arjun',99.77,108.99,999,1)\n",
    "   \n",
    "   x.execute(\"Select * from tweetMelbourne\")\n",
    "   rows = x.fetchall()\n",
    "   for row in rows:\n",
    "     print(row)\n",
    "except:\n",
    "   print \"hi\"\n",
    "   conn.rollback()\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tweet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-148-b8a793883efd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'%Y-%m-%d %H:%M:%S'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'created_at'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'%a %b %d %H:%M:%S +0000 %Y'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tweet' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
